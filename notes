- in current directory make a new folder named the series name - and make sure invalid charecters are not saved as folder name
- in that folder make cbz files for each chapter

if a folder in the name of the series already exists only download chapters that are not already in that folder
- first check if there isn't a folder in current directory named our series, then check if current directory itself is not named after our series - then make a new folder in current directory that is our series

use a folder, download pictures inside, create a cbr from files in folder
^call this folder working directory - create on every run, delete at end of every run

when download done - run this in working directory - run bach -c
zip ../(chapter name here).cbr *
rm -rf ./

if -v is applied print when a chapter finishes downloading
and also print each picture download like 3/36 and then zipping, and then done - make sure updates on same line with \r and if last message longer than next have enough spaces to remove it
don't forget to fflush

support kissmanga
and hitomi

gotta deal with cookies - keep a struct, and with challenge responses, do everytime 503 comes up, if after sending fails, ask again for webpage and don't send cookie info

do not fully impliment cookies - but put that as a thing to do, for now get it up and running :P

comment all methods
and global variables

also allow options like download from 000 ect.
and if they put in a chapter download from that chapter onwards ect.

allow for multiple urls
- loop through each argument and change variable accordingly
- make -v have to be last
- folder to save 2nd last if -v otherwise last

create a list of chapter structs then go through said structs

what if instead of series just a chapter page is given - then just download the chapter

clean up code!!

quick fail downloading chapters - and move on don't exit

keep auto-updating desired mangas - just run the url every day automatically or whatever

figure out images about joining scans and remove them - basiclly by hashing and comparing hashes, whatever repeats delete

-d disables deleting what it thinks Is doubled up data, makes faster, -e makes double up check extensive 
-hashmap of md5-hashes of pictures and location
-at first duplicate, unzip folder delete file, rezip, remove location data from hashmap, and delete the just downloaded file, if duplicate found and location is null, only delete new one
-try to keep the hashmap small, so after say 10 chapters all the hashes with locations in them, we can discard it, -e prevents this discard
-at the end we save the hashmap to a hidden file in the folder .duplicatehashset
-also provide a function, so you go into the folder and just run the program and it'll update the chapters - store settings in hidden file .settings
- if new url run in the folder - update the .settings file given the name of the magnga ends up the same

- keep some kind of auto updater?

-Make sure kissmanga side of things is kept seperate from the rest of the program

- custom md5 hash it or not?

- use a binary search tree for the hashmap so when I search if something alreayd exists I can do it in log n time!

thread the mapcheccking

-z means zip
-f means folder, keep the option
